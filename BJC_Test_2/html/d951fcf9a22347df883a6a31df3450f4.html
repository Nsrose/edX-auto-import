<h1>What is Algorithmic Complexity?</h1>
<h1>&nbsp;</h1>
<p>Algorithmic Complexity, also called Order of Growth or Running Time, refers to the number of steps a program takes as a function of the size of inputs. We will assume just one input <code> n </code></p>
<p>&nbsp;</p>
<h2>A Note on Notation</h2>
<h2>&nbsp;</h2>
<p>Aside from using "constant, linear, etc. " to describe orders of growth, we can also use "Big O" notation, a more mathamatical way of experessing running time where: <br /> <br />O(n) = Linear Growth <br />O(log n) = Logarithmic Growth <br />O(n<sup>2</sup>) = Quadratic Growth <br />O(n<sup>3</sup>) = Cubic Growth <br />O(2<sup>n</sup>) = Exponential Growth <br />... <br /> If you are still confused with what each of these looks like, do not fret! We will discuss them a bit closer in the next unit</p>
<p>&nbsp;</p>